# -*- coding: utf-8 -*-
"""
Created on Sat Oct 11 18:18:23 2025

@author: 6185
"""

# -*- coding: utf-8 -*-
import io
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, Tuple, Optional

import streamlit as st
import pandas as pd
import numpy as np

# ============== OPTIONAL DEPENDENCIES (guarded) ==============
try:
    import altair as alt
    ALTAIR_OK = True
except Exception:
    ALTAIR_OK = False

SKLEARN_OK = True
SKLEARN_ERR = ""
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.pipeline import Pipeline as SkPipeline
    from sklearn.linear_model import LogisticRegression
except Exception as e:
    SKLEARN_OK = False
    SKLEARN_ERR = str(e)

# ============== BASIC CONFIG ==============
st.set_page_config(page_title="–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω | Artel Support", layout="wide")
st.title("–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω (G1/G2/G3, D-–±–ª–æ–∫, UZS‚ÜíUSD, ML-—Å–∫–∏–¥–∫–∏)")
st.caption("–ó–∞–≥—Ä—É–∑–∫–∞ Excel –∏–∑ CRM ‚Üí –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Üí –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ ‚Üí —Å–≤–æ–¥—ã ‚Üí –≥–æ—Ç–æ–≤—ã–π Excel-–æ—Ç—á—ë—Ç")

MONEY_FORMAT_RU = '# ##0,00'  # Excel number format: 1 234 567,89

# ============== HELPERS (columns & formatting) ==============
def coalesce_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:
    """–°–∫–ª–µ–∏–≤–∞–µ–º –¥—É–±–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –Ω–µ–ø—É—Å—Ç–æ–µ —Å–ª–µ–≤–∞ –∏ —É–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏."""
    out = df.copy()
    counts = out.columns.value_counts()
    dup_names = counts[counts > 1].index.tolist()
    for name in dup_names:
        idxs = [i for i, c in enumerate(out.columns) if c == name]
        cols = [out.columns[i] for i in idxs]
        combined = out[cols].bfill(axis=1).iloc[:, 0]
        out[name] = combined
        for c in cols[1:]:
            out.drop(columns=c, inplace=True)
    return out

def existing_cols(df: pd.DataFrame, cols: list[str]) -> list[str]:
    return [c for c in cols if c in set(df.columns)]

def is_amount_col(colname: str) -> bool:
    lc = str(colname).lower()
    keys = ["sum", "total", "—Å–∫–∏–¥–∫", "—Å—É–º–º", "—Ü–µ–Ω–∞", "–¥–µ–ª—å—Ç–∞", "usd"]
    return any(k in lc for k in keys)

def add_totals_row_numeric(df: pd.DataFrame, label="–ò–¢–û–ì–û") -> pd.DataFrame:
    if df is None or df.empty:
        return df
    out = df.copy()
    num_cols = [c for c in out.columns if pd.api.types.is_numeric_dtype(out[c]) and is_amount_col(c)]
    if not num_cols:
        return out
    totals = {c: out[c].sum(skipna=True) for c in num_cols}
    label_col = None
    for c in ["plant_name","service_group","warranty_type","ticket_id","–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏"]:
        if c in out.columns:
            label_col = c
            break
    totals_row = {c: "" for c in out.columns}
    totals_row.update({c: totals[c] for c in num_cols})
    if label_col:
        totals_row[label_col] = label
    return pd.concat([out, pd.DataFrame([totals_row])], ignore_index=True)

def format_numbers_ru(df: pd.DataFrame) -> pd.DataFrame:
    """–§–æ—Ä–º–∞—Ç '# ##0,00' –¥–ª—è UI-—Ç–∞–±–ª–∏—Ü (—Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ)."""
    if df is None or df.empty:
        return df
    df_fmt = df.copy()
    for col in df_fmt.columns:
        if pd.api.types.is_numeric_dtype(df_fmt[col]):
            df_fmt[col] = df_fmt[col].apply(
                lambda x: f"{x:,.2f}".replace(",", "X").replace(".", ",").replace("X", " ")
                if pd.notnull(x) else ""
            )
    return df_fmt

# ============== APP CONFIG DATACLASS ==============
@dataclass
class PipelineConfig:
    src_sheet: str = "–î–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π"
    excluded_for_manufacturer: Tuple[str, ...] = ("–í–´–ó–û–í", "–ü–†–û–î–ê–ñ–ê")
    spare_parts_labels: Tuple[str, ...] = ("–ó–∞–ø—á–∞—Å—Ç—å", )
    ml_threshold: float = 0.85
    rename_map: Dict[str, str] = field(default_factory=lambda: {
        "–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏": "ticket_id",
        "–¢–∏–ø –≥–∞—Ä–∞–Ω—Ç–∏–∏": "warranty_type",
        "–ì—Ä—É–ø–ø–∞ —É—Å–ª—É–≥": "service_group",
        "–ì—Ä—É–ø–ø–∞ —É—Å–ª—É–≥ ": "service_group",
        "–£—Å–ª—É–≥–∞": "service_name",
        "–°—É–º–º–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É": "sum_product",
        "–°—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ": "sum_service",
        "–°—É–º–º–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É –ë–µ–∑ —Å–∫–∏–¥–∫–∏": "sum_product_before_disc",
        "–°—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ –ë–µ–∑ —Å–∫–∏–¥–∫–∏": "sum_service_before_disc",
        "–°–∫–∏–¥–∫–∞ –Ω–∞ —É—Å–ª—É–≥—É": "discount_service",
        "–°–∫–∏–¥–∫–∞ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç": "discount_product",
        "–∑–∞–≤–æ–¥": "plant_name",
        "–í–∏–¥ –º–∞—Ç–µ—Ä–∏–∞–ª–∞": "material_type",
        "–í–∏–¥ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ ": "material_type",
        "–ë—Ä–µ–Ω–¥": "brand",
        "–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è": "created_at",
        "–î–∞—Ç–∞ –ø—Ä–æ–¥–∞–∂–∏": "sold_at",
        "–°–µ—Ä–≤–∏—Å–Ω—ã–π —Ü–µ–Ω—Ç—Ä": "service_center",
        "–°–µ—Ä–≤–∏—Å–Ω—ã–π —Ü–µ–Ω—Ç—Ä ": "service_center",
        "–ö–æ–¥ –ø—Ä–æ–¥—É–∫—Ç–∞": "product_code",
        "–ü—Ä–æ–¥—É–∫—Ç": "product_name",
        "–¢–∏–ø –∑–∞—è–≤–∫–∏": "request_type",
        "–°—Ç–∞—Ç—É—Å –∑–∞—è–≤–∫–∏": "ticket_status",
        "–°—Ç–∞—Ç—É—Å –∑–∞—è–≤–∫–∏ ": "ticket_status",
        "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ": "tech_conclusion",
        "–°–ü–° 1": "sps1",
        "–°–ü–° 2": "sps2",
        "–¥–∞—Ç–∞ —Å–ø—Å": "sps_date",
        "–ö–æ–¥ —É—Å–ª—É–≥–∏": "service_code",
        "–°—Ç–æ–∏–º–æ—Å—Ç—å —É—Å–ª—É–≥–∏": "service_unit_price",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ª—É–≥": "qty_service",
        "–î–∞—Ç–∞ –æ–∫–∞–∑–∞–Ω–∏—è": "service_date",
        "–î–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "post_date",
    })
    required_cols: Tuple[str, ...] = (
        "ticket_id","warranty_type","service_group","sum_product","sum_service",
        "discount_service","discount_product","plant_name","material_type",
        "service_code","service_unit_price","qty_service","sps_date"
    )

# ============== SIDEBAR INPUTS ==============
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
sheet_name = st.sidebar.text_input("–ò–º—è –ª–∏—Å—Ç–∞ –≤ Excel", value="–î–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π")
excluded_types = st.sidebar.text_input("–ò—Å–∫–ª—é—á–∏—Ç—å –∏–∑ –∑–∞—Ç—Ä–∞—Ç –∑–∞–≤–æ–¥–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", value="–í–´–ó–û–í, –ü–†–û–î–ê–ñ–ê")
sp_labels = st.sidebar.text_input("–ú–µ—Ç–∫–∏ –∑–∞–ø—á–∞—Å—Ç–µ–π (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", value="–ó–∞–ø—á–∞—Å—Ç—å")

st.sidebar.markdown("---")
st.sidebar.subheader("–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –∏ ML —Å–∫–∏–¥–æ–∫")
rates_file = st.sidebar.file_uploader("–ö—É—Ä—Å UZS‚ÜíUSD (CSV/XLSX)", type=["csv", "xlsx"])
ml_training_file = st.sidebar.file_uploader("–ò—Å—Ç–æ—Ä–∏—è —Å–∫–∏–¥–æ–∫ —Å –º–µ—Ç–∫–∞–º–∏ (CSV/XLSX)", type=["csv", "xlsx"])
ml_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ ML (—Å–∫–∏–¥–∫–∏):", 0.5, 0.99, 0.85, 0.01)

st.sidebar.markdown("---")
st.sidebar.subheader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CRM –≤—ã–≥—Ä—É–∑–∫—É")
uploaded = st.sidebar.file_uploader("Excel –∏–∑ CRM (.xlsx)", type=["xlsx"])

# ============== CACHING HELPERS ==============
@st.cache_data(show_spinner=False)
def read_excel_cached(file_bytes: bytes, sheet: str) -> pd.DataFrame:
    df = pd.read_excel(io.BytesIO(file_bytes), sheet_name=sheet)
    return df

@st.cache_data(show_spinner=False)
def read_rates(file) -> pd.DataFrame:
    df = pd.read_csv(file) if file.name.lower().endswith(".csv") else pd.read_excel(file)
    df = coalesce_duplicate_columns(df)
    cols = {c.strip().lower(): c for c in df.columns}
    date_col = next((cols[k] for k in cols if k in ("date", "–¥–∞—Ç–∞")), None)
    rate_col = next((cols[k] for k in cols if k in ("rate", "–∫—É—Ä—Å", "uzs_usd", "uzs_to_usd")), None)
    if not date_col or not rate_col:
        raise ValueError("–í —Ñ–∞–π–ª–µ –∫—É—Ä—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'date/–î–∞—Ç–∞' –∏ 'rate/–ö—É—Ä—Å'.")
    df = df.rename(columns={date_col: "date", rate_col: "rate"})
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df["rate"] = pd.to_numeric(df["rate"], errors="coerce")
    df = df.dropna(subset=["date", "rate"]).sort_values("date")
    df = df.drop_duplicates(subset=["date"], keep="last")
    return df[["date","rate"]]

@st.cache_data(show_spinner=False)
def read_training_discounts(file) -> pd.DataFrame:
    df = pd.read_csv(file) if file.name.lower().endswith(".csv") else pd.read_excel(file)
    df = coalesce_duplicate_columns(df)
    low = {c.strip().lower(): c for c in df.columns}
    desc_col = next((low[k] for k in ("description","–æ–ø–∏—Å–∞–Ω–∏–µ","discount_desc","desc","–æ—Å–Ω–æ–≤–∞–Ω–∏–µ","–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π") if k in low), None)
    label_col = next((low[k] for k in ("approved_by","—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ","label","–º–µ—Ç–∫–∞","source","responsible") if k in low), None)
    if not desc_col or not label_col:
        raise ValueError("–í –æ–±—É—á–∞—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –Ω—É–∂–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ 'description/–æ–ø–∏—Å–∞–Ω–∏–µ' –∏ 'approved_by/—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ'.")
    df = df.rename(columns={desc_col: "description", label_col: "approved_by"})
    df["description"] = df["description"].astype(str).fillna("")
    df["approved_by"] = df["approved_by"].astype(str).str.strip().str.upper()
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —è—Ä–ª—ã–∫–æ–≤ (–ø—Ä–∏–º–µ—Ä)
    label_map = {
        "–ó–ê–í–û–î": "PLANT", "PLANT": "PLANT",
        "–°–¶": "SC", "SC": "SC", "SERVICE CENTER": "SC",
        "–î–ë–õ–û–ö": "SP", "–°–ü": "SP", "SPARE PARTS": "SP", "D-BLOCK": "SP"
    }
    df["approved_by"] = df["approved_by"].map(lambda x: label_map.get(x, x))
    df = df.loc[df["description"].str.strip().ne("")]
    df = df.drop_duplicates(subset=["description","approved_by"])
    return df[["description","approved_by"]]

@st.cache_resource(show_spinner=False)
def train_discount_classifier(train_df: pd.DataFrame):
    if not SKLEARN_OK:
        raise RuntimeError(f"ML-–º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {SKLEARN_ERR}")
    pipe = SkPipeline([
        ("tfidf", TfidfVectorizer(max_features=3000, ngram_range=(1,2))),
        ("clf", LogisticRegression(max_iter=300, solver="lbfgs", multi_class="auto"))
    ])
    pipe.fit(train_df["description"].values, train_df["approved_by"].values)
    return pipe

# ============== PIPELINE FUNCTIONS ==============
@dataclass
class PipelineConfigRuntime(PipelineConfig):
    pass

def load_and_normalize(file_bytes: bytes, cfg: PipelineConfigRuntime, sheet_name: Optional[str] = None) -> pd.DataFrame:
    df = read_excel_cached(file_bytes, sheet_name or cfg.src_sheet)
    df.columns = [c.strip() for c in df.columns]
    df = df.rename(columns={k: v for k, v in cfg.rename_map.items() if k in df.columns})
    df = coalesce_duplicate_columns(df)

    for col in cfg.required_cols:
        if col not in df.columns:
            df[col] = np.nan

    for col in ["sum_product","sum_service","discount_service","discount_product","service_unit_price","qty_service"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df["qty_service"] = df["qty_service"].fillna(1)
    df[["sum_product","sum_service","discount_service","discount_product","service_unit_price"]] = \
        df[["sum_product","sum_service","discount_service","discount_product","service_unit_price"]].fillna(0)

    for dcol in ["sps_date","service_date","post_date","sold_at","created_at"]:
        if dcol in df.columns:
            df[dcol] = pd.to_datetime(df[dcol], errors="coerce")

    df["service_group_up"] = df["service_group"].astype(str).str.upper().str.strip()
    df["warranty_type"] = df["warranty_type"].astype(str).str.upper().str.strip()
    df["material_type"] = df["material_type"].astype(str).str.strip()

    df["sum_total"] = df["sum_product"] + df["sum_service"]
    df["discount_total"] = df["discount_service"] + df["discount_product"]
    return df

def add_line_flags(df: pd.DataFrame, cfg: PipelineConfigRuntime) -> pd.DataFrame:
    df = df.copy()
    df["is_excluded_for_manufacturer"] = df["service_group_up"].isin([s.upper() for s in cfg.excluded_for_manufacturer])
    df["is_spare_part"] = df["material_type"].isin(cfg.spare_parts_labels)
    return df

def apply_latest_service_price(df: pd.DataFrame, cfg: PipelineConfigRuntime) -> tuple[pd.DataFrame, pd.DataFrame]:
    df = df.copy()
    elig = (df["warranty_type"].isin(["G1","G2"])) & (~df["is_spare_part"]) & df["service_code"].notna()
    df_elig = df.loc[elig].copy().sort_values(["service_code","sps_date"])
    latest_price_map = (
        df_elig.dropna(subset=["service_unit_price","sps_date"])
              .groupby("service_code")["service_unit_price"].last().to_dict()
    )
    old_price = df.loc[elig, "service_unit_price"].copy()
    new_price = df.loc[elig, "service_code"].map(latest_price_map).fillna(old_price)
    qty = df.loc[elig, "qty_service"].fillna(1)

    old_sum_service = df.loc[elig, "sum_service"].fillna(0).astype(float)
    old_sum_service_recalc = (old_price.fillna(0).astype(float) * qty.astype(float))
    old_sum_service_final = np.where(old_sum_service > 0, old_sum_service, old_sum_service_recalc)
    new_sum_service = (new_price.astype(float) * qty.astype(float))
    delta = new_sum_service - old_sum_service_final

    df.loc[elig, "service_unit_price"] = new_price
    df.loc[elig, "sum_service"] = new_sum_service
    df.loc[elig, "sum_total"] = df.loc[elig, "sum_product"] + df.loc[elig, "sum_service"]

    audit = df.loc[elig, ["ticket_id","service_code","plant_name","warranty_type","sps_date","qty_service"]].copy()
    audit["old_unit_price"] = old_price.values
    audit["new_unit_price"] = new_price.values
    audit["old_sum_service"] = old_sum_service_final
    audit["new_sum_service"] = new_sum_service
    audit["delta_sum_service"] = delta
    audit = audit[audit["delta_sum_service"] != 0]
    return df, audit

def apply_usd_conversion(df: pd.DataFrame, rates: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    working_date = df["sps_date"]
    if working_date.isna().all():
        for fallback in ["service_date","post_date","sold_at","created_at"]:
            if fallback in df.columns:
                wd = pd.to_datetime(df[fallback], errors="coerce")
                if wd.notna().any():
                    working_date = wd
                    break
    df["_rate_date"] = pd.to_datetime(working_date, errors="coerce")

    tmp = df[["_rate_date"]].copy()
    tmp["__row__"] = np.arange(len(tmp))
    tmp = tmp.sort_values("_rate_date")
    rates_sorted = rates.sort_values("date")

    merged = pd.merge_asof(tmp, rates_sorted, left_on="_rate_date", right_on="date", direction="backward")
    merged = merged.sort_values("__row__")
    df["usd_rate"] = merged["rate"].values

    for col in ["sum_product","sum_service","sum_total","discount_product","discount_service","discount_total"]:
        if col in df.columns:
            df[col + "_usd"] = np.where(df["usd_rate"] > 0, df[col] / df["usd_rate"], np.nan)
    return df

def aggregate_by_ticket(df: pd.DataFrame, use_usd: bool=False) -> pd.DataFrame:
    if use_usd:
        return (
            df.groupby("ticket_id").agg(
                sum_product=("sum_product_usd","sum"),
                sum_service=("sum_service_usd","sum"),
                sum_total=("sum_total_usd","sum"),
                discount_product=("discount_product_usd","sum"),
                discount_service=("discount_service_usd","sum"),
                discount_total=("discount_total_usd","sum"),
                warranty_type=("warranty_type","first"),
                plant_name=("plant_name","first")
            ).reset_index()
        )
    else:
        return (
            df.groupby("ticket_id").agg(
                sum_product=("sum_product","sum"),
                sum_service=("sum_service","sum"),
                sum_total=("sum_total","sum"),
                discount_product=("discount_product","sum"),
                discount_service=("discount_service","sum"),
                discount_total=("discount_total","sum"),
                warranty_type=("warranty_type","first"),
                plant_name=("plant_name","first")
            ).reset_index()
        )

def warranty_totals_from_id(agg_id: pd.DataFrame) -> pd.DataFrame:
    return (
        agg_id.groupby("warranty_type")[["sum_product","sum_service","sum_total","discount_total"]]
        .sum().reset_index()
    )

def ar_by_plant(df: pd.DataFrame, use_usd: bool=False) -> tuple[pd.DataFrame, pd.DataFrame]:
    mask = df["warranty_type"].isin(["G1","G2"]) & (~df["is_excluded_for_manufacturer"])
    cols = ["plant_name","warranty_type","ticket_id"]
    vals = ["sum_product_usd","sum_service_usd","sum_total_usd","discount_total_usd"] if use_usd \
           else ["sum_product","sum_service","sum_total","discount_total"]
    ar_lines = df.loc[mask, cols + vals]
    ar_id = (
        ar_lines.groupby(["plant_name","warranty_type","ticket_id"], as_index=False)
        .agg(**{vals[0]: (vals[0], "sum"),
                vals[1]: (vals[1], "sum"),
                vals[2]: (vals[2], "sum"),
                vals[3]: (vals[3], "sum")})
    )
    ar_summary = (
        ar_id.groupby(["plant_name","warranty_type"], as_index=False)
        .agg(**{vals[0]: (vals[0], "sum"),
                vals[1]: (vals[1], "sum"),
                vals[2]: (vals[2], "sum"),
                vals[3]: (vals[3], "sum")})
        .sort_values(vals[2], ascending=False)
    )
    return ar_id, ar_summary

def g3_views(df: pd.DataFrame, use_usd: bool=False) -> tuple[pd.DataFrame, pd.DataFrame]:
    cols_base = existing_cols(df, ["service_group","ticket_id","is_spare_part","plant_name"])
    vals = ["sum_product_usd","sum_service_usd","sum_total_usd","discount_total_usd"] if use_usd \
           else ["sum_product","sum_service","sum_total","discount_total"]
    vals = existing_cols(df, vals)
    g3_lines = df.loc[df["warranty_type"]=="G3", cols_base + vals].copy()
    if g3_lines.empty:
        return g3_lines, pd.DataFrame()
    g3_summary = (
        g3_lines.groupby("service_group", as_index=False)
        .agg(**{vals[0]: (vals[0],"sum"),
                vals[1]: (vals[1],"sum"),
                vals[2]: (vals[2],"sum"),
                vals[3]: (vals[3],"sum")})
        .sort_values(vals[2], ascending=False)
    )
    return g3_lines, g3_summary

def dblock_outputs(df: pd.DataFrame, use_usd: bool=False) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    mask_pay = df["is_spare_part"] & df["warranty_type"].isin(["G2","G3"])
    base_cols = existing_cols(df, ["plant_name","warranty_type","ticket_id","product_name","product_code"])

    if use_usd:
        vals_src = existing_cols(df, ["sum_product_usd","discount_product_usd"])
        pay_lines = df.loc[mask_pay, base_cols + vals_src].copy()
        pay_lines = pay_lines.rename(columns={"sum_product_usd":"sum_product","discount_product_usd":"discount_product"})
    else:
        vals_src = existing_cols(df, ["sum_product","discount_product"])
        pay_lines = df.loc[mask_pay, base_cols + vals_src].copy()

    # –°–≤–æ–¥
    sum_cols_ok = existing_cols(pay_lines, ["sum_product","discount_product"])
    if existing_cols(pay_lines, ["plant_name","warranty_type"]):
        pay_summary = (
            pay_lines.groupby(existing_cols(pay_lines, ["plant_name","warranty_type"]), as_index=False)
            .agg(**{
                "–°—É–º–º–∞_–∑–∞–ø—á–∞—Å—Ç–µ–π": (sum_cols_ok[0], "sum") if "sum_product" in sum_cols_ok else ("ticket_id", "size"),
                "–°–∫–∏–¥–∫–∏_–ø–æ_–∑–∞–ø—á–∞—Å—Ç—è–º_–∏–Ω—Ñ–æ": (sum_cols_ok[1], "sum") if "discount_product" in sum_cols_ok else ("ticket_id", "size"),
            })
            .sort_values("–°—É–º–º–∞_–∑–∞–ø—á–∞—Å—Ç–µ–π", ascending=False, na_position="last")
        )
    else:
        d = {}
        if "sum_product" in sum_cols_ok:
            d["–°—É–º–º–∞_–∑–∞–ø—á–∞—Å—Ç–µ–π"] = [pay_lines["sum_product"].sum()]
        if "discount_product" in sum_cols_ok:
            d["–°–∫–∏–¥–∫–∏_–ø–æ_–∑–∞–ø—á–∞—Å—Ç—è–º_–∏–Ω—Ñ–æ"] = [pay_lines["discount_product"].sum()]
        if not d:
            d["–°—É–º–º–∞_–∑–∞–ø—á–∞—Å—Ç–µ–π"] = [len(pay_lines)]
        pay_summary = pd.DataFrame(d)

    # –†–µ–µ—Å—Ç—Ä —Å–∫–∏–¥–æ–∫ –ø–æ –∑–∞–ø—á–∞—Å—Ç—è–º
    mask_disc = df["is_spare_part"] & (
        (df.get("discount_product", 0) > 0) | (df.get("discount_service", 0) > 0) |
        (df.get("discount_product_usd", 0) > 0) | (df.get("discount_service_usd", 0) > 0)
    )
    base_cols_disc = existing_cols(df, ["plant_name","warranty_type","ticket_id","product_name","product_code"])
    if use_usd:
        vals_disc = existing_cols(df, ["discount_product_usd","discount_service_usd","sum_product_usd","sum_service_usd","sum_total_usd"])
        disc_register = df.loc[mask_disc, base_cols_disc + vals_disc].copy().rename(columns={
            "discount_product_usd":"discount_product",
            "discount_service_usd":"discount_service",
            "sum_product_usd":"sum_product",
            "sum_service_usd":"sum_service",
            "sum_total_usd":"sum_total"
        })
    else:
        vals_disc = existing_cols(df, ["discount_product","discount_service","sum_product","sum_service","sum_total"])
        disc_register = df.loc[mask_disc, base_cols_disc + vals_disc].copy()

    return pay_lines, pay_summary, disc_register

def build_changed_prices_reports(df_before: pd.DataFrame, audit_price_adj: pd.DataFrame, cfg: PipelineConfigRuntime):
    changed_lines = audit_price_adj.rename(columns={
        "ticket_id":"–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏","service_code":"–ö–æ–¥ —É—Å–ª—É–≥–∏","plant_name":"–ó–∞–≤–æ–¥",
        "warranty_type":"–¢–∏–ø –≥–∞—Ä–∞–Ω—Ç–∏–∏","sps_date":"–î–∞—Ç–∞ –°–ü–°","qty_service":"–ö–æ–ª-–≤–æ —É—Å–ª—É–≥",
        "old_unit_price":"–°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞ —É—Å–ª—É–≥–∏","new_unit_price":"–ù–æ–≤–∞—è —Ü–µ–Ω–∞ —É—Å–ª—É–≥–∏",
        "old_sum_service":"–°—Ç–∞—Ä–∞—è —Å—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ","new_sum_service":"–ù–æ–≤–∞—è —Å—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ",
        "delta_sum_service":"–î–µ–ª—å—Ç–∞ (—Å—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ)"
    })
    cols_order = [
        "–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏","–ö–æ–¥ —É—Å–ª—É–≥–∏","–¢–∏–ø –≥–∞—Ä–∞–Ω—Ç–∏–∏","–ó–∞–≤–æ–¥","–î–∞—Ç–∞ –°–ü–°","–ö–æ–ª-–≤–æ —É—Å–ª—É–≥",
        "–°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞ —É—Å–ª—É–≥–∏","–ù–æ–≤–∞—è —Ü–µ–Ω–∞ —É—Å–ª—É–≥–∏",
        "–°—Ç–∞—Ä–∞—è —Å—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ","–ù–æ–≤–∞—è —Å—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ","–î–µ–ª—å—Ç–∞ (—Å—É–º–º–∞ –ø–æ —É—Å–ª—É–≥–µ)"
    ]
    if not changed_lines.empty:
        changed_lines = changed_lines[[c for c in cols_order if c in changed_lines.columns]] \
            .sort_values(["–ö–æ–¥ —É—Å–ª—É–≥–∏","–î–∞—Ç–∞ –°–ü–°","–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏"], ascending=[True, True, True])

    elig_before = (
        (df_before["warranty_type"].isin(["G1","G2"])) &
        (~df_before["material_type"].isin(cfg.spare_parts_labels)) &
        (df_before["service_code"].notna())
    )
    last_sps_date = (
        df_before.loc[elig_before, ["service_code","sps_date"]]
        .dropna(subset=["sps_date"])
        .groupby("service_code", as_index=False)["sps_date"].max()
        .rename(columns={"sps_date": "–ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –°–ü–°"})
    )

    def _mode_or_edge(s: pd.Series, edge: str = "last"):
        s = pd.Series(s)
        m = s.mode()
        if len(m): return m.iloc[0]
        return s.iloc[-1] if edge == "last" else s.iloc[0]

    if audit_price_adj.empty:
        changed_summary = pd.DataFrame(columns=["–ö–æ–¥ —É—Å–ª—É–≥–∏","–°—Ç–∞—Ä–∞—è_—Ü–µ–Ω–∞","–ù–æ–≤–∞—è_—Ü–µ–Ω–∞",
                                                "–ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –°–ü–°","–°—Ç—Ä–æ–∫_–∑–∞—Ç—Ä–æ–Ω—É—Ç–æ","–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö_–∑–∞—è–≤–æ–∫","–î–µ–ª—å—Ç–∞_—Å—É–º–º–∞"])
        return changed_summary, changed_lines

    pairs = (
        audit_price_adj.groupby("service_code", as_index=False)
        .agg(
            –°—Ç–∞—Ä–∞—è_—Ü–µ–Ω–∞=("old_unit_price", lambda x: _mode_or_edge(x, "first")),
            –ù–æ–≤–∞—è_—Ü–µ–Ω–∞=("new_unit_price", lambda x: _mode_or_edge(x, "last")),
            –°—Ç—Ä–æ–∫_–∑–∞—Ç—Ä–æ–Ω—É—Ç–æ=("service_code","count"),
            –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö_–∑–∞—è–≤–æ–∫=("ticket_id", pd.Series.nunique),
            –î–µ–ª—å—Ç–∞_—Å—É–º–º–∞=("delta_sum_service","sum"),
        )
        .rename(columns={"service_code":"–ö–æ–¥ —É—Å–ª—É–≥–∏"})
    )
    changed_summary = pairs.merge(
        last_sps_date.rename(columns={"service_code":"–ö–æ–¥ —É—Å–ª—É–≥–∏"}),
        on="–ö–æ–¥ —É—Å–ª—É–≥–∏", how="left"
    )[["–ö–æ–¥ —É—Å–ª—É–≥–∏","–°—Ç–∞—Ä–∞—è_—Ü–µ–Ω–∞","–ù–æ–≤–∞—è_—Ü–µ–Ω–∞","–ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –°–ü–°","–°—Ç—Ä–æ–∫_–∑–∞—Ç—Ä–æ–Ω—É—Ç–æ","–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö_–∑–∞—è–≤–æ–∫","–î–µ–ª—å—Ç–∞_—Å—É–º–º–∞"]] \
     .sort_values(["–ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –°–ü–°","–ö–æ–¥ —É—Å–ª—É–≥–∏"], ascending=[False, True])
    return changed_summary, changed_lines

def ensure_ticket_id_text(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    if "ticket_id" in out.columns:
        def _to_text(x):
            if pd.isna(x): return ""
            try:
                as_num = pd.to_numeric(x, errors="coerce")
                return str(int(as_num)) if pd.notna(as_num) else str(x)
            except Exception:
                return str(x)
        out["ticket_id"] = out["ticket_id"].apply(_to_text)
    if "–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏" in out.columns:
        out["–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏"] = out["–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏"].astype(str)
    return out

def _colnum_to_excel(n: int) -> str:
    s, n = "", n + 1
    while n:
        n, r = divmod(n-1, 26)
        s = chr(65 + r) + s
    return s

def write_sheets_to_bytes(sheets: Dict[str, Optional[pd.DataFrame]]) -> bytes:
    """–ü–∏—à–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ DataFrame –Ω–∞ –ª–∏—Å—Ç—ã: —Ñ–æ—Ä–º–∞—Ç –¥–µ–Ω–µ–≥, —Ç–µ–∫—Å—Ç–æ–≤—ã–π ticket_id, –∞–≤—Ç–æ—à–∏—Ä–∏–Ω–∞, —Ñ–æ—Ä–º—É–ª—ã –ò–¢–û–ì–û."""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        wb = writer.book
        fmt_money = wb.add_format({'num_format': MONEY_FORMAT_RU})
        fmt_text  = wb.add_format({'num_format': '@'})
        fmt_bold  = wb.add_format({'bold': True})
        fmt_bold_money = wb.add_format({'bold': True, 'num_format': MONEY_FORMAT_RU})

        for sheet_name, df in sheets.items():
            if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                continue
            df_to_write = ensure_ticket_id_text(df)
            df_to_write.to_excel(writer, sheet_name=sheet_name, index=False)
            ws = writer.sheets[sheet_name]

            # –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ ‚Äî –¥–µ–Ω—å–≥–∏
            money_cols_idx = []
            for i, c in enumerate(df_to_write.columns):
                if is_amount_col(c) and pd.api.types.is_numeric_dtype(df_to_write[c]):
                    money_cols_idx.append(i)

            # —Ñ–æ—Ä–º–∞—Ç—ã –∏ —à–∏—Ä–∏–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            for i, name in enumerate(df_to_write.columns):
                if name in ("ticket_id", "–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏"):
                    ws.set_column(i, i, 18, fmt_text)
                elif i in money_cols_idx:
                    ws.set_column(i, i, 16, fmt_money)
                else:
                    ws.set_column(i, i, 12)

            # autofit —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–æ–≤
            for i, name in enumerate(df_to_write.columns):
                vals = df_to_write.iloc[:, i].astype(str).head(200).tolist()
                width = min(max(10, max(len(str(name)), *(len(v) for v in vals)) + 2), 42)
                keep_fmt = fmt_text if name in ("ticket_id","–ù–æ–º–µ—Ä –∑–∞—è–≤–∫–∏") else (fmt_money if i in money_cols_idx else None)
                ws.set_column(i, i, width, keep_fmt)

            # —Å—Ç—Ä–æ–∫–∞ –ò–¢–û–ì–û
            nrows = len(df_to_write)
            if nrows > 0 and money_cols_idx:
                total_row_excel = nrows + 1
                label_col = 0
                for j, name in enumerate(df_to_write.columns):
                    if j not in money_cols_idx:
                        label_col = j; break
                ws.write(total_row_excel, label_col, "–ò–¢–û–ì–û", fmt_bold)
                for j in money_cols_idx:
                    col_letter = _colnum_to_excel(j)
                    formula = f"=SUM({col_letter}2:{col_letter}{nrows+1})"
                    ws.write_formula(total_row_excel, j, formula, fmt_bold_money)

    output.seek(0)
    return output.getvalue()

# ============== UI FILTERS ==============
def apply_ui_filters(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        plants = ["(–≤—Å–µ)"] + sorted([p for p in df.get("plant_name", pd.Series(dtype=str)).dropna().astype(str).unique()])
        plant_sel = st.selectbox("–§–∏–ª—å—Ç—Ä: –∑–∞–≤–æ–¥", plants)
        if plant_sel != "(–≤—Å–µ)":
            df = df[df["plant_name"] == plant_sel]
    with c2:
        brands = ["(–≤—Å–µ)"] + sorted([b for b in df.get("brand", pd.Series(dtype=str)).dropna().astype(str).unique()])
        brand_sel = st.selectbox("–§–∏–ª—å—Ç—Ä: –±—Ä–µ–Ω–¥", brands)
        if brand_sel != "(–≤—Å–µ)" and "brand" in df.columns:
            df = df[df["brand"] == brand_sel]
    with c3:
        warranties = ["(–≤—Å–µ)", "G1", "G2", "G3"]
        w_sel = st.selectbox("–§–∏–ª—å—Ç—Ä: –≥–∞—Ä–∞–Ω—Ç–∏—è", warranties)
        if w_sel != "(–≤—Å–µ)":
            df = df[df["warranty_type"] == w_sel]
    with c4:
        if "sps_date" in df.columns and df["sps_date"].notna().any():
            min_d = pd.to_datetime(df["sps_date"]).min()
            max_d = pd.to_datetime(df["sps_date"]).max()
            start, end = st.date_input("–ü–µ—Ä–∏–æ–¥ –ø–æ '–¥–∞—Ç–∞ —Å–ø—Å'", value=(min_d.date(), max_d.date()))
            if start and end:
                mask = (pd.to_datetime(df["sps_date"]).dt.date >= start) & (pd.to_datetime(df["sps_date"]).dt.date <= end)
                df = df[mask]
    return df

# ============== MAIN FLOW ==============
cfg = PipelineConfigRuntime(
    src_sheet=sheet_name,
    excluded_for_manufacturer=tuple([s.strip().upper() for s in excluded_types.split(",") if s.strip()]),
    spare_parts_labels=tuple([s.strip() for s in sp_labels.split(",") if s.strip()]),
    ml_threshold=ml_threshold
)

if uploaded is None:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª CRM (—Ñ–æ—Ä–º–∞—Ç .xlsx) –≤ —Å–∞–π–¥–±–∞—Ä–µ, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")
    st.stop()

try:
    with st.spinner("–ß–∏—Ç–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ..."):
        df = load_and_normalize(uploaded.getvalue(), cfg, sheet_name=sheet_name)
        df = add_line_flags(df, cfg)
        df_before = df.copy()

    st.markdown("### –§–∏–ª—å—Ç—Ä—ã –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞")
    df_preview = apply_ui_filters(df)

    with st.spinner("–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ü–µ–Ω—ã G1/G2 –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º '–¥–∞—Ç–∞ —Å–ø—Å'..."):
        df, audit_price_adj = apply_latest_service_price(df, cfg)
        changed_summary, changed_lines = build_changed_prices_reports(df_before, audit_price_adj, cfg)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ USD (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    usd_enabled = False
    if rates_file is not None:
        with st.spinner("–ü—Ä–∏–º–µ–Ω—è–µ–º –∫—É—Ä—Å—ã UZS‚ÜíUSD..."):
            rates = read_rates(rates_file)
            df = apply_usd_conversion(df, rates)
            usd_enabled = True
    else:
        st.info("–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã ‚Äî USD-–∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.")

    # –ê–≥—Ä–µ–≥–∞—Ü–∏–∏ (UZS)
    with st.spinner("–°—Ç—Ä–æ–∏–º —Å–≤–æ–¥—ã (UZS)..."):
        agg_id = aggregate_by_ticket(df, use_usd=False)
        warranty_totals = warranty_totals_from_id(agg_id)
        ar_id, ar_summary = ar_by_plant(df, use_usd=False)
        g3_lines, g3_summary = g3_views(df, use_usd=False)
        dblock_pay_lines, dblock_pay_summary, dblock_disc_register = dblock_outputs(df, use_usd=False)

    # –ê–≥—Ä–µ–≥–∞—Ü–∏–∏ (USD), –µ—Å–ª–∏ –µ—Å—Ç—å –∫—É—Ä—Å—ã
    agg_id_usd = warranty_totals_usd = ar_id_usd = ar_summary_usd = None
    g3_lines_usd = g3_summary_usd = dblock_pay_lines_usd = dblock_pay_summary_usd = dblock_disc_register_usd = None
    if usd_enabled:
        with st.spinner("–°—Ç—Ä–æ–∏–º —Å–≤–æ–¥—ã (USD)..."):
            agg_id_usd = aggregate_by_ticket(df, use_usd=True)
            warranty_totals_usd = warranty_totals_from_id(agg_id_usd)
            ar_id_usd, ar_summary_usd = ar_by_plant(df, use_usd=True)
            g3_lines_usd, g3_summary_usd = g3_views(df, use_usd=True)
            dblock_pay_lines_usd, dblock_pay_summary_usd, dblock_disc_register_usd = dblock_outputs(df, use_usd=True)

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–∫–∏–¥–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    confident_ml = review_ml = pd.DataFrame()
    if ml_training_file is not None and SKLEARN_OK:
        with st.spinner("–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å–∫–∏–¥–æ–∫ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –∫ —Ç–µ–∫—É—â–∏–º –¥–∞–Ω–Ω—ã–º..."):
            train_df = read_training_discounts(ml_training_file)
            model = train_discount_classifier(train_df)

            def classify_discounts(current_df: pd.DataFrame, model, threshold: float = 0.85):
                dfc = current_df.copy()
                text_col = None
                for candidate in ["discount_description","–æ–ø–∏—Å–∞–Ω–∏–µ —Å–∫–∏–¥–∫–∏","–æ–ø–∏—Å–∞–Ω–∏–µ","description","desc","sps1","sps2","service_name"]:
                    if candidate in dfc.columns:
                        text_col = candidate
                        break
                if text_col is None:
                    return pd.DataFrame(), pd.DataFrame()
                has_disc = ((dfc.get("discount_product", 0) > 0) | (dfc.get("discount_service", 0) > 0))
                cand = dfc.loc[has_disc].copy()
                if cand.empty:
                    return pd.DataFrame(), pd.DataFrame()
                texts = cand[text_col].astype(str).fillna("")
                try:
                    proba = model.predict_proba(texts)
                    classes = model.classes_
                    pred_idx = np.argmax(proba, axis=1)
                    cand["ML_–ú–µ—Ç–∫–∞"] = classes[pred_idx]
                    cand["ML_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] = proba[np.arange(len(cand)), pred_idx]
                except Exception:
                    pred = model.predict(texts)
                    cand["ML_–ú–µ—Ç–∫–∞"] = pred
                    cand["ML_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] = np.nan
                confident_df = cand.loc[cand["ML_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] >= cfg.ml_threshold].copy()
                review_df = cand.loc[(cand["ML_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] < cfg.ml_threshold) | (cand["ML_–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"].isna())].copy()
                return confident_df, review_df

            confident_ml, review_ml = classify_discounts(df, model, threshold=cfg.ml_threshold)
    else:
        if ml_training_file is None:
            st.info("–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å–∫–∏–¥–æ–∫ ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        elif not SKLEARN_OK:
            st.info(f"ML-–º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {SKLEARN_ERR}. –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–±–æ—Ç—É –±–µ–∑ ML.")

    # ==================== PREVIEW & DASHBOARD ====================
    st.success("–ì–æ—Ç–æ–≤–æ! –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–≤–æ–¥–æ–∫ –Ω–∏–∂–µ, –∞ —Ç–∞–∫–∂–µ –∫–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è Excel.")

    # KPI (UZS)
    st.markdown("### üìä –î–∞—à–±–æ—Ä–¥ ‚Äî –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
    k1, k2, k3, k4 = st.columns(4)
    safe_sum = lambda d, c: float(pd.to_numeric(d.get(c, pd.Series(dtype=float)), errors="coerce").sum())
    k1.metric("–ò—Ç–æ–≥–æ –æ–±–æ—Ä–æ—Ç (UZS)", f"{safe_sum(df,'sum_total'):,.2f}".replace(",", "X").replace(".", ",").replace("X", " "))
    k2.metric("–°–∫–∏–¥–∫–∏ –≤—Å–µ–≥–æ (UZS)", f"{safe_sum(df,'discount_total'):,.2f}".replace(",", "X").replace(".", ",").replace("X", " "))
    k3.metric("–£—Å–ª—É–≥–∏ (UZS)", f"{safe_sum(df,'sum_service'):,.2f}".replace(",", "X").replace(".", ",").replace("X", " "))
    k4.metric("–ú–∞—Ç–µ—Ä–∏–∞–ª—ã (UZS)", f"{safe_sum(df,'sum_product'):,.2f}".replace(",", "X").replace(".", ",").replace("X", " "))

    # KPI (USD)
    if "sum_total_usd" in df.columns:
        k5, k6, k7, k8 = st.columns(4)
        k5.metric("–ò—Ç–æ–≥–æ –æ–±–æ—Ä–æ—Ç (USD)", f"{safe_sum(df,'sum_total_usd'):,.2f}")
        k6.metric("–°–∫–∏–¥–∫–∏ (USD)", f"{safe_sum(df,'discount_total_usd'):,.2f}")
        k7.metric("–£—Å–ª—É–≥–∏ (USD)", f"{safe_sum(df,'sum_service_usd'):,.2f}")
        k8.metric("–ú–∞—Ç–µ—Ä–∏–∞–ª—ã (USD)", f"{safe_sum(df,'sum_product_usd'):,.2f}")

    # –ì—Ä–∞—Ñ–∏–∫–∏
    st.markdown("### üìà –ì—Ä–∞—Ñ–∏–∫–∏")
    if ALTAIR_OK:
        c1c, c2c = st.columns(2)
        with c1c:
            if {"plant_name","warranty_type","sum_total"}.issubset(df.columns):
                top_plants = (
                    df[df["warranty_type"].isin(["G1","G2"])]
                    .groupby("plant_name", as_index=False)["sum_total"].sum()
                    .sort_values("sum_total", ascending=False).head(10)
                )
                if not top_plants.empty:
                    st.altair_chart(
                        alt.Chart(top_plants).mark_bar().encode(
                            x=alt.X("sum_total:Q", title="–û–±–æ—Ä–æ—Ç (UZS)"),
                            y=alt.Y("plant_name:N", sort='-x', title="–ó–∞–≤–æ–¥"),
                            tooltip=["plant_name","sum_total"]
                        ).properties(height=320),
                        use_container_width=True
                    )
        with c2c:
            if {"service_group","warranty_type","sum_total"}.issubset(df.columns):
                g3_grp = (
                    df[df["warranty_type"]=="G3"]
                    .groupby("service_group", as_index=False)["sum_total"].sum()
                    .sort_values("sum_total", ascending=False).head(10)
                )
                if not g3_grp.empty:
                    st.altair_chart(
                        alt.Chart(g3_grp).mark_bar().encode(
                            x=alt.X("sum_total:Q", title="–û–±–æ—Ä–æ—Ç G3 (UZS)"),
                            y=alt.Y("service_group:N", sort='-x', title="–ì—Ä—É–ø–ø–∞ —É—Å–ª—É–≥"),
                            tooltip=["service_group","sum_total"]
                        ).properties(height=320),
                        use_container_width=True
                    )
        if "sps_date" in df.columns and df["sps_date"].notna().any():
            ts = (
                df.dropna(subset=["sps_date"])
                  .assign(day=lambda d: pd.to_datetime(d["sps_date"]).dt.date)
                  .groupby("day", as_index=False)["sum_total"].sum()
                  .sort_values("day")
            )
            if not ts.empty:
                st.altair_chart(
                    alt.Chart(ts).mark_line(point=True).encode(
                        x=alt.X("day:T", title="–î–∞—Ç–∞ –°–ü–°"),
                        y=alt.Y("sum_total:Q", title="–û–±–æ—Ä–æ—Ç (UZS)"),
                        tooltip=["day","sum_total"]
                    ).properties(height=300),
                    use_container_width=True
                )
    else:
        st.info("Altair –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –≥—Ä–∞—Ñ–∏–∫–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã.")

    # –¢–∞–±–ª–∏—Ü—ã (UI, —Å –ò–¢–û–ì–û –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    c1t, c2t = st.columns(2)
    with c1t:
        st.subheader("–ò—Ç–æ–≥–∏ –ø–æ –≥–∞—Ä–∞–Ω—Ç–∏–∏ (UZS)")
        st.dataframe(format_numbers_ru(add_totals_row_numeric(warranty_totals).head(200)))
        st.subheader("AR –ø–æ –∑–∞–≤–æ–¥–∞–º ‚Äî —Å–≤–æ–¥ (UZS)")
        st.dataframe(format_numbers_ru(add_totals_row_numeric(ar_summary).head(200)))
        if usd_enabled:
            st.subheader("–ò—Ç–æ–≥–∏ –ø–æ –≥–∞—Ä–∞–Ω—Ç–∏–∏ (USD)")
            st.dataframe(format_numbers_ru(add_totals_row_numeric(warranty_totals_usd).head(200)))
            st.subheader("AR –ø–æ –∑–∞–≤–æ–¥–∞–º ‚Äî —Å–≤–æ–¥ (USD)")
            st.dataframe(format_numbers_ru(add_totals_row_numeric(ar_summary_usd).head(200)))
    with c2t:
        st.subheader("G3 ‚Äî —Å–≤–æ–¥ (UZS)")
        st.dataframe(format_numbers_ru(add_totals_row_numeric(g3_summary).head(200)))
        if usd_enabled:
            st.subheader("G3 ‚Äî —Å–≤–æ–¥ (USD)")
            st.dataframe(format_numbers_ru(add_totals_row_numeric(g3_summary_usd).head(200)))
        st.subheader("–ò–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ü–µ–Ω—ã (—Å–≤–æ–¥)")
        st.dataframe(format_numbers_ru(add_totals_row_numeric(changed_summary).head(200)))
        if not confident_ml.empty:
            st.subheader("–°–∫–∏–¥–∫–∏ (ML, —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)")
            st.dataframe(format_numbers_ru(confident_ml.head(200)))
        if not review_ml.empty:
            st.subheader("–°–∫–∏–¥–∫–∏ (–Ω–∞ —Ä—É—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É)")
            st.dataframe(format_numbers_ru(review_ml.head(200)))

    # ==================== EXCEL EXPORT ====================
    sheets = {
        "–ò—Ç–æ–≥–∏_–ø–æ_ID": agg_id,
        "–ò—Ç–æ–≥–∏_–ø–æ_–ì–∞—Ä–∞–Ω—Ç–∏–∏": warranty_totals,
        "–ê–†_–ø–æ_–ó–∞–≤–æ–¥–∞–º_ID": ar_id,
        "–ê–†_–ø–æ_–ó–∞–≤–æ–¥–∞–º_–°–≤–æ–¥": ar_summary,
        "–ì3_–°—Ç—Ä–æ–∫–∏": g3_lines,
        "–ì3_–°–≤–æ–¥": g3_summary,
        "–î–ë–ª–æ–∫_–ö—Ä–µ–¥–∏—Ç–æ—Ä–∫–∞_–°—Ç—Ä–æ–∫–∏": dblock_pay_lines,
        "–î–ë–ª–æ–∫_–ö—Ä–µ–¥–∏—Ç–æ—Ä–∫–∞_–°–≤–æ–¥": dblock_pay_summary,
        "–î–ë–ª–æ–∫_–°–∫–∏–¥–∫–∏": dblock_disc_register,
        "G1G2_–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞_–¢–∞—Ä–∏—Ñ–æ–≤": audit_price_adj,
        "G1G2_–ò–∑–º–µ–Ω–µ–Ω–Ω—ã–µ_–¶–µ–Ω—ã_–°–≤–æ–¥": changed_summary,
        "G1G2_–ò–∑–º–µ–Ω–µ–Ω–Ω—ã–µ_–¶–µ–Ω—ã_–°—Ç—Ä–æ–∫–∏": changed_lines,
    }
    if usd_enabled:
        sheets.update({
            "–ò—Ç–æ–≥–∏_–ø–æ_ID_USD": agg_id_usd,
            "–ò—Ç–æ–≥–∏_–ø–æ_–ì–∞—Ä–∞–Ω—Ç–∏–∏_USD": warranty_totals_usd,
            "–ê–†_–ø–æ_–ó–∞–≤–æ–¥–∞–º_ID_USD": ar_id_usd,
            "–ê–†_–ø–æ_–ó–∞–≤–æ–¥–∞–º_–°–≤–æ–¥_USD": ar_summary_usd,
            "–ì3_–°—Ç—Ä–æ–∫–∏_USD": g3_lines_usd,
            "–ì3_–°–≤–æ–¥_USD": g3_summary_usd,
            "–î–ë–ª–æ–∫_–ö—Ä–µ–¥–∏—Ç–æ—Ä–∫–∞_–°—Ç—Ä–æ–∫–∏_USD": dblock_pay_lines_usd,
            "–î–ë–ª–æ–∫_–ö—Ä–µ–¥–∏—Ç–æ—Ä–∫–∞_–°–≤–æ–¥_USD": dblock_pay_summary_usd,
            "–î–ë–ª–æ–∫_–°–∫–∏–¥–∫–∏_USD": dblock_disc_register_usd,
        })
    if not confident_ml.empty:
        sheets["–°–∫–∏–¥–∫–∏_–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"] = confident_ml
    if not review_ml.empty:
        sheets["–°–∫–∏–¥–∫–∏_–ö_–ü—Ä–æ–≤–µ—Ä–∫–µ"] = review_ml

    xlsx_bytes = write_sheets_to_bytes(sheets)
    st.download_button(
        label="üíæ –°–∫–∞—á–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π Excel-–æ—Ç—á—ë—Ç",
        data=xlsx_bytes,
        file_name="–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ_–∏—Ç–æ–≥–∏_–º–µ—Å—è—Ü–∞.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

except Exception as e:
    st.error(f"–û—à–∏–±–∫–∞: {e}")
